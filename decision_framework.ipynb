{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c2bad6",
   "metadata": {},
   "source": [
    "현진: 호기심\n",
    "원준: 플래그/폴리시\n",
    "나: 예측 기반 오차/오류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측기반 오차\n",
    "# var.\n",
    "state = {\n",
    "    'state_num': -1,   # 현재 state 기준 과거는 -1, 미래는 +1\n",
    "    'reward': 1\n",
    "}\n",
    "\n",
    "def prophecy(state): #prophecy는 신탁\n",
    "    score = 0\n",
    "    \n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def reward(prophecy_score_bef = prophecy(state), state_reward_bef):\n",
    "    w=10\n",
    "    return abs(prophecy_score_bef - state_reward_bef) * w # w는 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a6969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAG 보상 함수\n",
    "def flag_reward(flag_str, method, known_flags, found_paths):\n",
    "    \"\"\"\n",
    "    플래그를 찾았을 때 보상을 부여하는 함수 (구조만 설계함)\n",
    "    - flag_str: 발견된 플래그 문자열\n",
    "    - method: 플래그를 찾은 탐색 경로(명령/행동 이름)\n",
    "    - known_flags: 미리 등록된 정답 플래그 해시 목록(dict)\n",
    "    - found_paths: 이미 찾은 플래그 및 경로 기록용 dict\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    # 1) 해시 비교\n",
    "    flag_hash = hash(flag_str)\n",
    "    for key, val in known_flags.items():\n",
    "        if flag_hash == val:\n",
    "            # 2) 이미 찾았던 플래그인가?\n",
    "            if method not in found_paths.get(key, []):\n",
    "                reward = 5   # 기본 보상\n",
    "                reward += 2  # 새로운 경로로 찾은 보너스\n",
    "                found_paths.setdefault(key, []).append(method)\n",
    "            else:\n",
    "                reward = 5   # 동일 경로는 기본 보상만\n",
    "            break\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAG 보상 함수 (기틀 버전)\n",
    "def flag_reward(flag_str, method, known_flags, found_paths):\n",
    "    \"\"\"\n",
    "    플래그를 찾았을 때 보상을 부여하는 함수 (구조만 설계함)\n",
    "    - flag_str: 발견된 플래그 문자열\n",
    "    - method: 플래그를 찾은 탐색 경로(명령/행동 이름)\n",
    "    - known_flags: 미리 등록된 정답 플래그 해시 목록(dict)\n",
    "    - found_paths: 이미 찾은 플래그 및 경로 기록용 dict\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    # 1) 해시 비교 \n",
    "    flag_hash = hash(flag_str)\n",
    "    for key, val in known_flags.items():\n",
    "        if flag_hash == val:\n",
    "            # 2) 이미 찾았던 플래그인가?\n",
    "            if method not in found_paths.get(key, []):\n",
    "                reward = 5   # 기본 보상\n",
    "                reward += 2  # 새로운 경로로 찾은 보너스\n",
    "                found_paths.setdefault(key, []).append(method)\n",
    "            else:\n",
    "                reward = 5   # 동일 경로는 기본 보상만\n",
    "            break\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf51222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy (ε-탐욕 기반)\n",
    "def policy_update(actions, Q, epsilon=0.1, alpha=0.1, last_action=None, reward=0):\n",
    "    \"\"\"\n",
    "    간단한 ε-탐욕 정책 함수 \n",
    "    - actions: 가능한 행동 리스트\n",
    "    - Q: 행동 가치 테이블 (dict)\n",
    "    - epsilon: 무작위 탐색 확률\n",
    "    - alpha: 학습률\n",
    "    - last_action: 직전 수행한 행동\n",
    "    - reward: 직전 보상\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    # Q값 업데이트 (최근 보상 반영)\n",
    "    if last_action is not None:\n",
    "        old_value = Q.get(last_action, 0)\n",
    "        Q[last_action] = old_value + alpha * (reward - old_value)\n",
    "\n",
    "    # ε-탐욕 정책으로 다음 행동 선택\n",
    "    if random.random() < epsilon:\n",
    "        next_action = random.choice(actions)\n",
    "    else:\n",
    "        next_action = max(actions, key=lambda a: Q.get(a, 0))\n",
    "\n",
    "    return next_action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd32",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
